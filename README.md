# Transformer_Architecture_from_Scratch

![MasterHead](https://assets-global.website-files.com/6305e5d52c28356b4fe71bac/63f8df8c87f95232f94ad05c_Holistic-AI-Figure-2.gif)
## Overview

This repository contains an implementation of the Transformer architecture, a powerful model for natural language processing (NLP) tasks. The Transformer model was introduced in the paper "Attention is All You Need" by Vaswani et al. This implementation provides a modular and flexible framework for building and training Transformer-based models for various NLP tasks.

## Features

- **Modularity**: The code is structured into modular components, allowing easy customization and extension.
- **Configurability**: Hyperparameters and model configurations are stored in a centralized configuration file for easy experimentation.
- **Flexibility**: The code supports multiple variants of the Transformer architecture, such as encoder-only, decoder-only, or both.
- **Data Processing**: Includes data preprocessing and tokenization scripts for common NLP datasets.

## Getting Started

### Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/AkshayPethe/Transformer_Architecture.git
    cd Transformer_Architecture
    ```

2. Install dependencies:

    ```bash
    pip install -r requirements.txt
    ```
